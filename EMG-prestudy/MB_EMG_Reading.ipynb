{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "#import pyvttbl as pt\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "#import preprocess2\n",
    "#import prepare\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from scipy.stats.mstats import zscore\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')  \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "import mne\n",
    "import imageio\n",
    "import datetime as dt\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder='./mb'\n",
    "behav_dir='./mb/VIT/Verhaltensdaten'\n",
    "output_filename_behav='./mb/mb_behav_new.csv'\n",
    "\n",
    "folder='./mb'\n",
    "EMG_dir='./mb/VIT/EMG'\n",
    "output_filename_EMG='./mb/mb_EMG.csv'\n",
    "\n",
    "video_dir='./mb/VIT/videos'\n",
    "output_filename_video='./mb/mb_video.csv'\n",
    "\n",
    "def remove():\n",
    "    to_delete=[output_filename_behav, output_filename_video, output_filename_EMG]\n",
    "\n",
    "    for files in to_delete:\n",
    "        try:\n",
    "            os.remove(os.path.join(files))\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_EMG(output_filename_EMG, EMG_dir):\n",
    "    outputf=open(output_filename_EMG, \"a\") # create and open the outputfile\n",
    "\n",
    "    # input folder and list of input files \n",
    "    EMG_files = os.listdir(EMG_dir)\n",
    "        \n",
    "    #go through all input files\n",
    "    for filecount, filename in enumerate(EMG_files):\n",
    "        path=os.path.join(EMG_dir, filename)\n",
    "        inputf=open(path)\n",
    "        for c, line in enumerate(inputf): # write all lines of each file into the final one  \n",
    "            if (filecount==0) & (c==8):\n",
    "                outputf.write('vpn'   + '\\t' + 'frame' + '\\t' + 'time' + '\\t' + 'delta' + '\\t' + line)\n",
    "            if (c==2):\n",
    "                time=line\n",
    "            if (c==4):\n",
    "                delta=line #print line #4 delts                 \n",
    "            if(c>8):\n",
    "                outputf.write(repr(filename) + '\\t' + repr(c) + '\\t' + repr(time) + '\\t' + repr(delta) + '\\t'  + line)\n",
    "        inputf.close() \n",
    "    outputf.close()\n",
    "\n",
    "read_EMG(output_filename_EMG, EMG_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_EMG_datafile(filename):\n",
    "    emg_data=pd.read_csv(filename, sep=\"\\t\",  parse_dates=True, na_values=['?'])   \n",
    "\n",
    "    emg_data.columns = ['vpn', 'frame', 'time', 'delta', 'trigger', 'SCL', 'SCR', 'EMG1',\n",
    "       'EMG2', 'EMG3', 'Unnamed']\n",
    "    emg_data['vpn']=emg_data['vpn'].str[4:6] \n",
    "    emg_data['timestamp']=pd.to_numeric(emg_data['frame'])-8\n",
    "    emg_data['rec_date']=pd.to_datetime(emg_data['time'].str[22:42])\n",
    "    emg_data['rec_delta']=pd.to_numeric(emg_data['delta'].str[22:26])\n",
    "    emg_data=emg_data.drop(columns=['Unnamed', 'frame', 'time', 'delta'])\n",
    "    emg_data=emg_data.rename(columns={'EMG1': 'ZYG', 'EMG2': 'COR', 'EMG3' : 'LEV'}) #name muscles\n",
    "    return emg_data\n",
    "\n",
    "\n",
    "def get_data_of_vpn(vpn_emg, vpn):\n",
    "    vpn_emg=vpn_emg[vpn_emg.vpn==vpn].reset_index(drop=True) #get participants data\n",
    "    start=vpn_emg.rec_date[1] #get recording date\n",
    "    lines=len(vpn_emg) #get number of records\n",
    "    vpn_emg.loc[:,'time'] = pd.date_range(start, periods=lines, freq='0.05S') # get sampling indec\n",
    "    vpn_emg.loc[:,'vpn'] = pd.to_numeric(vpn) # get vpn\n",
    "    return vpn_emg\n",
    "\n",
    "def get_conv_parts_and_times(vpn_emg):\n",
    "    vpn_emg.loc[:, 'conversation_start']=np.max(vpn_emg[vpn_emg.trigger==100]['timestamp'])\n",
    "    vpn_emg.loc[:,'neutral_speaker_start']=np.max(vpn_emg[vpn_emg.trigger==102]['timestamp'])\n",
    "    vpn_emg.loc[:,'neutral_proband_start']=np.max(vpn_emg[vpn_emg.trigger==103]['timestamp'])\n",
    "    vpn_emg.loc[:,'joy_speaker_start']=np.max(vpn_emg[vpn_emg.trigger==104]['timestamp'])\n",
    "    vpn_emg.loc[:,'joy_proband_start']=np.max(vpn_emg[vpn_emg.trigger==105]['timestamp'])\n",
    "    vpn_emg.loc[:,'disgust_speaker_start']=np.max(vpn_emg[vpn_emg.trigger==106]['timestamp'])\n",
    "    vpn_emg.loc[:,'disgust_proband_start']=np.max(vpn_emg[vpn_emg.trigger==107]['timestamp'])\n",
    "    \n",
    "    vpn_emg['pretesting']=(vpn_emg.timestamp<vpn_emg.conversation_start)\n",
    "    vpn_emg['intro']=(vpn_emg.timestamp>=vpn_emg.conversation_start) & (vpn_emg.timestamp<vpn_emg.neutral_speaker_start)\n",
    "    \n",
    "    vpn_emg['neutral_speaker']=(vpn_emg.timestamp>=vpn_emg.neutral_speaker_start) & (vpn_emg.timestamp<vpn_emg.neutral_proband_start)\n",
    "    vpn_emg['neutral_proband']=(vpn_emg.timestamp>=vpn_emg.neutral_proband_start) & (vpn_emg.timestamp<vpn_emg.joy_speaker_start)\n",
    "    vpn_emg['joy_speaker']=(vpn_emg.timestamp>=vpn_emg.joy_speaker_start) & (vpn_emg.timestamp<vpn_emg.joy_proband_start)\n",
    "    vpn_emg['joy_proband']=(vpn_emg.timestamp>=vpn_emg.joy_proband_start) & (vpn_emg.timestamp<vpn_emg.disgust_speaker_start)\n",
    "\n",
    "    vpn_emg['disgust_speaker']=(vpn_emg.timestamp>=vpn_emg.disgust_speaker_start) & (vpn_emg.timestamp<vpn_emg.disgust_proband_start)\n",
    "    vpn_emg['disgust_proband']=(vpn_emg.timestamp>=vpn_emg.disgust_proband_start)\n",
    "\n",
    "    return vpn_emg\n",
    "\n",
    "def convert_conv_times_in_daytime(vpn_emg):\n",
    "    conversation_start_points=['conversation_start', 'neutral_speaker_start',\n",
    "                               'neutral_proband_start','joy_speaker_start','joy_proband_start',\n",
    "                               'disgust_speaker_start', 'disgust_proband_start']\n",
    "    vpn_emg=vpn_emg.reset_index(drop=True)\n",
    "    for conv in conversation_start_points:\n",
    "        timemark_in_s=pd.Timedelta((vpn_emg.iloc[0,:].rec_delta*vpn_emg.iloc[0,:][conv]), unit='s')\n",
    "        vpn_emg[(conv+'_dt')]=vpn_emg.iloc[0,:].time+timemark_in_s\n",
    "\n",
    "    return vpn_emg\n",
    "\n",
    " # if downsampled=2\n",
    "def cut_start(df):\n",
    "    seconds=1.5\n",
    "    samples_persec=20\n",
    "    delete=(pd.to_datetime(df.time)<(pd.to_datetime(df.reset_index(drop=True).conversation_start_dt[0])-pd.Timedelta(seconds=seconds)))\n",
    "    cut_df=df.loc[delete==False].reset_index(drop=True) #loc\n",
    "    return cut_df\n",
    "\n",
    "def cut_end(df):\n",
    "    seconds=42\n",
    "    delete= pd.to_datetime(df.time)> (pd.to_datetime(df.reset_index(drop=True).disgust_proband_start_dt[0])+pd.Timedelta(seconds=seconds))\n",
    "    cut_df=df.loc[delete==False].reset_index(drop=True) #loc added\n",
    "    return cut_df\n",
    "\n",
    "\n",
    "def outlier_trials(vpn_emg, outl):\n",
    "    muscles=['ZYG', 'COR', 'LEV']\n",
    "    for m in muscles:\n",
    "        \n",
    "        values=vpn_emg[m]\n",
    "        vpn_mean=np.nanmean(values, axis=0)\n",
    "        vpn_std=np.nanstd(values, axis=0)\n",
    "\n",
    "        if outl=='sd':\n",
    "            error=np.abs(values-vpn_mean) # es ist unsinn die absoluten Werte der Differenz zu nehmen!!\n",
    "            out_ind=(np.abs(values-vpn_mean))>(3*vpn_std)\n",
    "\n",
    "        if outl=='iq':\n",
    "            quartile_1, quartile_3 = np.percentile(values, [25, 75])\n",
    "            iqr = quartile_3 - quartile_1\n",
    "            lower_bound = quartile_1 - (iqr * 3)\n",
    "            upper_bound = quartile_3 + (iqr * 3)\n",
    "            out_ind=(values > upper_bound) | (values < lower_bound)\n",
    "\n",
    "        values[out_ind]=np.nan  \n",
    " \n",
    "        vpn_emg[m]=values.fillna(method='ffill')\n",
    "        \n",
    "    return vpn_emg   \n",
    "\n",
    "\n",
    "def dummycode_conv_parts(df):\n",
    "    conversation_parts=['pretesting', 'intro', 'neutral_speaker', 'neutral_proband', 'joy_proband',\n",
    "               'joy_speaker', 'disgust_speaker', 'disgust_proband']\n",
    "\n",
    "    #round them to avoid half/conversations !\n",
    "    df[conversation_parts]=np.round(df[conversation_parts])\n",
    "              \n",
    "    #categorical encoding\n",
    "    dummies=df[conversation_parts]\n",
    "    df[\"conversation\"] = dummies.idxmax(axis=1)\n",
    "    \n",
    "    #include a counter of Frames from the Begininng of the Interaction\n",
    "    df['counter']=df.groupby(['vpn', 'conversation']).cumcount() \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def zscore_muscles(vpn_emg):\n",
    "    muscles=['ZYG', 'COR', 'LEV']\n",
    "    for m in muscles:\n",
    "        vpn_emg[(m+'_z')]=zscore(vpn_emg[m])\n",
    "        \n",
    "    return vpn_emg\n",
    "\n",
    "def baseline_calculation(vpn_emg, basewindow):\n",
    "    \n",
    "    vpn_emg=vpn_emg.reset_index()\n",
    "    vpn_emg['index']=pd.to_numeric(vpn_emg['index'])\n",
    "    \n",
    "    muscles=['ZYG', 'COR', 'LEV']\n",
    "    for con in np.unique(vpn_emg.conversation):\n",
    "        if (con!='pretesting'): \n",
    "            ## achtung das wird für jeden Trigger gemacht und nicht nur einmal !!!\n",
    "            \n",
    "            trigger_index=np.int(vpn_emg[(vpn_emg.conversation==con) & (vpn_emg.counter==0)]['index'])\n",
    "            base_index=np.int(trigger_index-basewindow)\n",
    "\n",
    "\n",
    "            for m in muscles:\n",
    "                base_value=np.nanmean(vpn_emg.iloc[(base_index):(trigger_index)][m])\n",
    "                vpn_emg.loc[(vpn_emg.conversation==con), ('base_' + str(m))]=base_value\n",
    "                \n",
    "            #calculate number of broken trials\n",
    "            vpn_emg['broken']=sum(vpn_emg[muscles].mean(axis=1).isnull())\n",
    "            vpn_emg.iloc[(base_index):(trigger_index)]['conversation']='base' \n",
    "            \n",
    "    return vpn_emg\n",
    "\n",
    "def baseline_correction(vpn_emg):\n",
    "    muscles=['ZYG', 'COR', 'LEV']\n",
    "    for m in muscles:\n",
    "        vpn_emg[(m+'_basecorrected')]=vpn_emg[m]-vpn_emg[('base_' + str(m))]\n",
    "        #alternative: vpn_emg[(m+'_basecorrected')]=vpn_emg[m]/vpn_emg[('base_' + str(m))]\n",
    "    return vpn_emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emg_data=get_EMG_datafile(output_filename_EMG)\n",
    "#emg_data=final\n",
    "\n",
    "\n",
    "prepro_emgs = pd.DataFrame()\n",
    "# accelerati\n",
    "\n",
    "\n",
    "for vpn in np.unique(emg_data.vpn):\n",
    "    #get information\n",
    "    vpn_emg=get_data_of_vpn(emg_data, vpn)\n",
    "\n",
    "    vpn_emg_with_times=get_conv_parts_and_times(vpn_emg)    \n",
    "    vpn_emg_with_dt_times=convert_conv_times_in_daytime(vpn_emg_with_times)\n",
    "\n",
    "    #base sind -100 downsampled\n",
    "    vpn_emg_with_dt_timest_old=vpn_emg_with_dt_times.copy()\n",
    "    \n",
    "    vpn_emg_with_times_firstcut=cut_start(vpn_emg_with_dt_times)\n",
    "    print vpn_emg_with_dt_timest_old.equals(vpn_emg_with_times_firstcut)\n",
    "    \n",
    "    vpn_emg_with_times_firstcut_old=vpn_emg_with_times_firstcut.copy()  \n",
    "    vpn_emg_with_times_cutted=cut_end(vpn_emg_with_times_firstcut)\n",
    "    print vpn_emg_with_times_firstcut_old.equals(vpn_emg_with_times_cutted)\n",
    "    \n",
    "    vpn_emg_with_times_cutted_old=vpn_emg_with_times_cutted.copy()\n",
    "    vpn_emg_with_times_without_artefacts=outlier_trials(vpn_emg_with_times_cutted, 'sd')\n",
    "    print vpn_emg_with_times_without_artefacts.equals(vpn_emg_with_times_cutted_old)\n",
    "    #outlier-detection\n",
    "    \n",
    "    #downsampling of EMG\n",
    "    sample_rate=500\n",
    "    downsampled_emg=vpn_emg_with_times_without_artefacts.resample((str(sample_rate)+'ms'), on='time').mean() #downsample\n",
    "    downsampled_emg=downsampled_emg.reset_index() \n",
    "     \n",
    "    #conversation_time_formating\n",
    "    downsampled_emg_dt=convert_conv_times_in_daytime(downsampled_emg)\n",
    "    downsampled_emg_dt_conv=dummycode_conv_parts(downsampled_emg_dt)\n",
    "\n",
    "    #Baseline-Calculation\n",
    "    basewindow_in_seconds=2\n",
    "    basewindow=(1000/sample_rate)*basewindow_in_seconds\n",
    "    downsampled_emg_dt_conv_base=baseline_calculation(downsampled_emg_dt_conv, basewindow)\n",
    "    \n",
    "    #Baseline-Correction\n",
    "    downsampled_emg_dt_conv_base_old=downsampled_emg_dt_conv_base.copy()\n",
    "    prepro_emg=baseline_correction(downsampled_emg_dt_conv_base)\n",
    "    print downsampled_emg_dt_conv_base_old.equals(prepro_emg)\n",
    "    \n",
    "    #Zstandardisation within subject for each muscle [does it make sense to z-standardise?]\n",
    "    #prepro_emg_z=zscore_muscles(prepro_emg) #['seperate variables']\n",
    "    \n",
    "    prepro_emg=prepro_emg[prepro_emg.conversation!='pretesting']\n",
    "    \n",
    "    #Combine VPN with whole data set\n",
    "    prepro_emgs=pd.concat([prepro_emgs, prepro_emg], axis=0)\n",
    "    \n",
    "    \n",
    "final_emg=pd.DataFrame(prepro_emgs)#, columns=emg_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_emg.to_csv(output_filename_EMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_filename_EMG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Video-Information and Task-Times and Combine it wih EMG-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_behavioral(output_filename_behav, behav_dir):\n",
    "    outputf=open(output_filename_behav, \"a\") # create and open the outputfile\n",
    "\n",
    "    # input folder and list of input files \n",
    "    behav_files = os.listdir(behav_dir)\n",
    "\n",
    "    try:\n",
    "        os.remove(os.path.join(behav_dir, \".DS_Store\"))\n",
    "    except:\n",
    "        print(\"ok\")\n",
    "        \n",
    "    #go through all input files\n",
    "    for filecount, filename in enumerate(behav_files):\n",
    "        path=os.path.join(behav_dir, filename)\n",
    "        inputf=open(path)\n",
    "        print path\n",
    "        if filecount>0:\n",
    "            inputf.next()\n",
    "        for c, line in enumerate(inputf): # write all lines of each file into the final one        \n",
    "            if (filecount==0) & (c==0):\n",
    "                outputf.write('vpn' + ',' + line)\n",
    "            else:\n",
    "                outputf.write(repr(filename) + ',' + line)\n",
    "\n",
    "        inputf.close() \n",
    "    outputf.close()\n",
    "\n",
    "    \n",
    "def read_video_info(video_dir, output_filename_video):\n",
    "    \n",
    "    outputf=open(output_filename_video, \"a\") # create and open the outputfile\n",
    "    video_files = os.listdir(video_dir)\n",
    "\n",
    "    #go through all input files\n",
    "    for filecount, filename in enumerate(video_files):\n",
    "        path=os.path.join(video_dir, filename)\n",
    "\n",
    "        vid = imageio.get_reader(path,  'ffmpeg')\n",
    "\n",
    "        num_frames=vid._meta['nframes']     # number of frames in video\n",
    "        fps = vid._meta['fps']             # framerate of video\n",
    "\n",
    "        #created = dt.datetime.fromtimestamp(os.stat(path).st_birthtime)\n",
    "        created = dt.datetime.fromtimestamp(os.stat(path).st_mtime) #notwendig fürProbanden 80-83\n",
    "\n",
    "        \n",
    "        duration= num_frames/fps\n",
    "        start=created - pd.Timedelta(seconds=duration)\n",
    "\n",
    "        if filecount==0:\n",
    "            outputf.write('vpn' + ',' + 'start' + ',' + 'end' + ',' + 'num_frames' + ',' + 'fps' + '\\n')\n",
    "\n",
    "        outputf.write(repr(filename) + ',' + str(start) + ',' + str(created) +  ',' + str(num_frames) +  ',' + str(fps) + '\\n')\n",
    "\n",
    "    outputf.close()\n",
    "    \n",
    "    \n",
    "def merge_behav_video(output_filename_behav, output_filename_video):\n",
    "        \n",
    "    behav_data=pd.read_csv(output_filename_behav, sep=',', na_values=['?'])   \n",
    "    \n",
    "    #load video-information-file\n",
    "    video_data=pd.read_csv(output_filename_video, sep=',',  parse_dates=True, na_values=['?'])   \n",
    "\n",
    "    columns=['vpn', 'timer_neutral', 'timer_neutral_end', 'timer_joy',            \n",
    "                  'timer_joy_end', 'timer_disgust', 'timer_disgust_end']       \n",
    "    \n",
    "    behav_data.vpn=behav_data.vpn.str[4:6]\n",
    "    video_data.vpn=video_data.vpn.str[4:6]\n",
    "\n",
    "    behav_video_data=pd.merge(behav_data, video_data, on='vpn')\n",
    "    behav_video_data.rename(columns=lambda x: x.replace(\" \", \"\"), inplace=True) # exclude whitespaces\n",
    "\n",
    "    return behav_video_data\n",
    "\n",
    "\n",
    "def merge_all_together(behav_video_data, output_filename_EMG):\n",
    "    \n",
    "    #load feature_data\n",
    "    emg_data=pd.read_csv(output_filename_EMG, sep=\",\",  parse_dates=True, na_values=['?'])   \n",
    "    #emg_data['vpn']=emg_data['vpn'].str[4:6] \n",
    "    behav_video_data['vpn']=pd.to_numeric(behav_video_data.vpn)\n",
    "    # Merge together the behavioral/video-data with the video-feature.data\n",
    "    final=pd.merge(behav_video_data, emg_data, on='vpn')\n",
    "    final.rename(columns=lambda x: x.replace(\" \", \"\"), inplace=True) # exclude whitespaces\n",
    "    # drop columns\n",
    "    \n",
    "    #CHECKS!!!\n",
    "    print len(emg_data)\n",
    "    print len(behav_video_data)\n",
    "    print len(final)\n",
    "    \n",
    "    print len(set(behav_video_data.vpn))\n",
    "    print len(set(emg_data.vpn))#values\n",
    "    \n",
    "    print len(set(behav_video_data.vpn))==len(set(emg_data.vpn))#values\n",
    "    #ACHTUNG NUMBER 53 war  falsch gelabelt (einmal 33, einmal 11 --> korrekt ist 33) \n",
    "    \n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_behavioral(output_filename_behav, behav_dir)\n",
    "\n",
    "read_video_info(video_dir, output_filename_video)\n",
    "\n",
    "behav_video_data=merge_behav_video(output_filename_behav, output_filename_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=merge_all_together(behav_video_data, output_filename_EMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combination with Questionnaire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demographic(df_without_demographics):\n",
    "    demograph=pd.read_csv('./mb/demographic_data.csv', sep=',', na_values=['?']) \n",
    "    \n",
    "    demograph['vpn']=demograph.VPN.astype(int)\n",
    "    df_without_demographics['vpn']=df_without_demographics.vpn.astype(int)\n",
    "      \n",
    "   \n",
    "    df_with_demographics=pd.merge(df_without_demographics, demograph, on='vpn')\n",
    "    return df_with_demographics\n",
    "\n",
    "def online(df_without_online):\n",
    "    online=pd.read_csv('./mb/online.csv', sep=';', na_values=['?']) \n",
    "    \n",
    "    online['vpn']=online.VPN.astype(int)\n",
    "    online['pizza']=online.EV01_01\n",
    "    online['hering']=online.EV01_02\n",
    "    \n",
    "    df_without_online['vpn']=df_without_online.vpn.astype(int)\n",
    "    \n",
    "    df_with_online=pd.merge(df_without_online, online[['vpn', 'pizza', 'hering']], on='vpn')\n",
    "    return df_with_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_plus_demographics=demographic(final)\n",
    "final_plus_demographics_and_online=online(final_plus_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=final_plus_demographics_and_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('final_df_mb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hier stimmt noch etwas nicht: Die Colums existieren nicht mehr. Nochmal überarbeiten!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final=final[final.vpn!=26]\n",
    "def checks(final):\n",
    "    print 'intro'\n",
    "    print np.min(final['neutral_speaker_start']-final['conversation_start']) #3766.0\n",
    "    print np.max(final['neutral_speaker_start']-final['conversation_start']) #3807    #10485.0 vpn (26)\n",
    "\n",
    "    print 'neutral_speaker'\n",
    "    print np.min(final['neutral_proband_start']-final['neutral_speaker_start']) #554.0\n",
    "    print np.max(final['neutral_proband_start']-final['neutral_speaker_start']) #557 \n",
    "    print 'neutral_proband'\n",
    "    print np.min(final['neutral_proband_start']-final['neutral_speaker_start']) \n",
    "    print np.max(final['neutral_proband_start']-final['neutral_speaker_start']) \n",
    "\n",
    "    print 'joy_speaker'\n",
    "    print np.min(final['joy_speaker_start']-final['neutral_proband_start']) \n",
    "    print np.max(final['joy_speaker_start']-final['neutral_proband_start'])  \n",
    "    print 'joy_proband'\n",
    "    print np.min(final['joy_proband_start']-final['joy_speaker_start']) \n",
    "    print np.max(final['joy_proband_start']-final['joy_speaker_start']) \n",
    "\n",
    "    print 'disgust_speaker'\n",
    "    print np.min(final['disgust_speaker_start']-final['joy_proband_start']) \n",
    "    print np.max(final['disgust_speaker_start']-final['joy_proband_start']) \n",
    "\n",
    "    print 'disgust_proband'\n",
    "    print np.min(final['disgust_proband_start']-final['disgust_speaker_start']) \n",
    "    print np.max(final['disgust_proband_start']-final['disgust_speaker_start']) \n",
    "\n",
    "    print 'end'\n",
    "    print np.max(final['disgust_speaker_start'].reset_index(drop=True)) \n",
    "    \n",
    "    for con in np.unique(df.conversation):\n",
    "    print con\n",
    "    for v in np.unique(df.vpn):\n",
    "        print len(df[df.conversation==con][df.vpn==v])\n",
    "\n",
    "#multiplicated with 0.05 --> sec.\n",
    "checks(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusion based on Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_final=df.copy()\n",
    "df=df[df.vpn!=26] #  Probleme mit Lautsprecher/Mikro\n",
    "df=df[df.vpn!=33] # hat VIT erst nicht gemacht - hier dürfte sonst erst der zweite Durchgang gewertet werden\n",
    "\n",
    "df=df[df.conversation=='base'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(vpn_emg):\n",
    "    \n",
    "    muscles=['ZYG', 'COR', 'LEV']\n",
    "    for m in muscles:\n",
    "        vpn_emg[(m+'_mean')]=np.nanmean(vpn_emg[m])    \n",
    "        vpn_emg[(m+'_max')]=np.max(vpn_emg[m])\n",
    "        vpn_emg[(m+'_min')]=np.min(vpn_emg[m])  \n",
    "        vpn_emg[(m+'_var')]=np.var(vpn_emg[m])\n",
    "        vpn_emg[(m+'_skew')]=np.skew(vpn_emg[m])\n",
    "        vpn_emg[(m+'_kurtosis')]=np.kurtosis(vpn_emg[m])\n",
    "     \n",
    "    return\n",
    "    #hätte man auch über groupby and apply function machen können\n",
    "    #könnte man auch nochmal nur nach conversation-parts sich anschauen\n",
    "    #Mimicry-Mesearues: Difference between ZYG-COR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RESTE\n",
    "final=final[(final.timestamp>final.conversation_start)]  \n",
    "final.loc[((final.conversation=='ekel_proband')&(final.counter>900)), 'deletion']=1\n",
    "final=final[final.deletion!=1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
